// main.nf

nextflow.enable.dsl = 2

params.resources_dir = "/home/elukyamuzi/vector_hap/resources"
params.phased_vcfs = "/home/elukyamuzi/vector_hap/results/sample_phasing/*.gz"
params.phased_vcf_indexes = "/home/elukyamuzi/vector_hap/results/sample_phasing/*.gz.tbi"
params.interval_lists_dir = "${params.resources_dir}"
params.results = "/home/elukyamuzi/vector_hap/results"
params.project_id = "validation"
params.chromosome_list = ['2L', '2R', '3L', '3R', 'X']
//params.chromosome_list = ['2L']

include { MergeVcfs } from '../modules/MergeVcfs'
include { BgzipAndTabixII } from '../modules/BgzipAndTabixII'
include { SHAPEIT } from '../modules/SHAPEIT'
include { TabixII } from '../modules/TabixII'
include {LigateRegions} from '../modules/LigateRegions'
include { cohortVcfToZarr } from '../modules/cohortVcfToZarr'

workflow {

    // Run bcftools to merge phased VCF files into a single multi-sample VCF
    tuple_ch = channel.fromPath(params.phased_vcfs, checkIfExists:true) \
        | map { phased_vcf ->
          def fileName = phased_vcf.simpleName.toString()
          def parts = fileName.tokenize('_')
          def lastUnderscoreIndex = fileName.lastIndexOf('_')
          def contig = fileName.substring(lastUnderscoreIndex + 1) // Extract contig from the part after the last underscore
          return tuple(contig, phased_vcf)
        } \
        | groupTuple() 
          
     merge_ch = MergeVcfs(tuple_ch, file(params.phased_vcf_indexes))
     
    //merge_ch.view()

    // Run BgzipAndTabix to compress and index the VCF files generated by SelectVariants
    compress_ch2 = BgzipAndTabixII(merge_ch.merged_vcf)
    //compress_ch2.view()

    // Define an empty channel to merge all region channels into
    def merged_regions_ch

    // Loop through each contig and create a channel for each interval list
    params.chromosome_list.each { contig ->
        interval_list_path = "${params.interval_lists_dir}/intervals_gamb_colu_arab_${contig}_200000_40000.txt"
        region_ch = Channel.fromPath(interval_list_path)
                          .splitText()
                          .map { it.trim() }
                          .map { tuple(it, contig) }
        if (!merged_regions_ch) {
            merged_regions_ch = region_ch
        } else {
            merged_regions_ch = merged_regions_ch.mix(region_ch)
        }
    } 

    // View the merged regions channel
    //merged_regions_ch.view()
  
    // Run shapeit for cohort phasing
    //shapeit_ch = SHAPEIT(merged_regions_ch, compress_ch2)
    shapeit_ch = merged_regions_ch.map { region, contig ->
        def merged_vcf_contig = "${params.project_id}_${contig}_merged.vcf.gz"
        def merged_vcf_contig_path = file(params.results + '/cohort_phasing/' + merged_vcf_contig)
        def merged_vcf_index_path = file("${merged_vcf_contig_path}.tbi")
        tuple(region, contig, merged_vcf_contig_path, merged_vcf_index_path)
    } | SHAPEIT

    // Run Tabix to index the phased VCF files by genomic region
    //index_ch2 = TabixII(shapeit_ch)
    index_ch2 = shapeit_ch.map { region, vcf_file, log_file -> tuple(region, vcf_file) } | TabixII
    //tabix_ch2 = shapeit_tuple.out.map { region, phased_vcf -> Tabix}.view()
    
    // Extract the VCF files from shapeit_ch
    phased_vcfs_ch = shapeit_ch.map { region, vcf_file, log_file -> tuple(region, vcf_file) }

    // Combine phased_vcfs and index_files into tuples
    combined_tuples = phased_vcfs_ch.combine(index_ch2, by: 0)

    // Pass the phased VCF files and their indexes to LigateRegions to concatenate phased chunks
    //shapeit_ch.map { it -> [ it[1]]  } | view()
    //combine_ch = phased_vcfs_ch.combine(index_ch2).view()
    //ligate_ch.view()
    ligate_ch = LigateRegions(combined_tuples)

    //Run cohortVcfToZarr to convert the final VCF to Zarr 
    //cohortVcfToZarr(ligate_ch.take(1)) 
    cohortVcfToZarr(ligate_ch.first())

}
